---
title: "Stat823"
author: "Oluwafunmibi&Kena"
date: "2024-11-17"
output: pdf_document
---

```{r setup, include=FALSE}
# Load required libraries
library(dplyr)
library(tidyr)
library(corrplot)
library(ggplot2)
library(lightgbm)
library(caret)
library(Metrics)
library(missForest)
library(vcd) # For Cramer's V
library(GA)
```

# Read the data
```{r}
train_data <- read.csv("train_data.csv")
test_data <- read.csv("test_data.csv")
```

# Check structure
```{r}
str(train_data)
```

# Define variable types
```{r}
continuous_vars <- c("ann_prm_amt", "newest_veh_age", "home_lot_sq_footage", 
                     "household_policy_counts", "X12m_call_history", "tenure_at_snapshot")
categorical_vars <- c("acq_method", "bi_limit_group", "channel", "digital_contact_ind", 
                      "geo_group", "has_prior_carrier", "household_group", "pay_type_code", 
                      "pol_edeliv_ind", "prdct_sbtyp_grp", "product_sbtyp", "telematics_ind", 
                      "trm_len_mo")
target_var <- "call_counts"
```

```{r}
train_data$newest_veh_age <- ifelse(train_data$newest_veh_age == -20, NA, train_data$newest_veh_age)
train_data$telematics_ind <- ifelse(train_data$telematics_ind == -2 | train_data$telematics_ind == 0, NA, train_data$telematics_ind)
train_data$pol_edeliv_ind <- ifelse(train_data$pol_edeliv_ind == -2, NA, train_data$pol_edeliv_ind)
```

# Handle missing values in Test
```{r}
test_data$newest_veh_age <- ifelse(test_data$newest_veh_age == -20, NA, test_data$newest_veh_age)
test_data$telematics_ind <- ifelse(test_data$telematics_ind == -2 | test_data$telematics_ind == 0, NA, test_data$telematics_ind)
test_data$pol_edeliv_ind <- ifelse(test_data$pol_edeliv_ind == -2, NA, test_data$pol_edeliv_ind)
```

# Ensure categorical variables are set as factors
```{r}
train_data[categorical_vars] <- lapply(train_data[categorical_vars], as.factor)
test_data[categorical_vars] <- lapply(test_data[categorical_vars], as.factor)
```

# Calculate % missing
```{r}
missing_percentage <- sapply(train_data, function(x) mean(is.na(x)) * 100)
missing_percentage

```


```{r}
# Calculate % zeros for response variable (call_counts)
zero_percentage_call_counts <- mean(train_data$call_counts == 0) * 100
zero_percentage_call_counts

```

```{r}

continuous_associations <- sapply(continuous_vars, function(var) cor(train_data[[var]], train_data$call_counts, use = "complete.obs"))
continuous_associations

```
# Chi-square association for each categorical variable
```{r}
categorical_associations <- sapply(categorical_vars, function(var) {
  table_var <- table(train_data[[var]], train_data$call_counts)
  cramer_v <- assocstats(table_var)$cramer
  cramer_v
})
categorical_associations

```
# Perform Random Forest imputation using missForest
```{r}
imputed_data <- missForest(train_data, maxiter = 10, ntree = 100, variablewise = TRUE, decreasing = TRUE)
imputed_train_data <- as.data.frame(imputed_data$ximp)
```

# Perform KNN imputation on the test data 
```{r}
preProc_test <- preProcess(test_data, method = "knnImpute")

# Impute missing values in the test data
test_imputed <- predict(preProc_test, test_data)

imputed_data_test <- missForest(test_data, maxiter = 10, ntree = 100, variablewise = TRUE, decreasing = TRUE)
imputed_data_test <- as.data.frame(imputed_data_test$ximp)
```

# Split the train data into training and validation sets (80-20 split)
```{r}
set.seed(123)
trainIndex <- createDataPartition(imputed_train_data$call_counts, p = 0.8, list = FALSE)
trainSet <- imputed_train_data[trainIndex, ]
validSet <- imputed_train_data[-trainIndex, ]
```

```{r}
# Prepare data for LightGBM
train_x <- trainSet %>% select(-call_counts)
train_y <- trainSet$call_counts
valid_x <- validSet %>% select(-call_counts)
valid_y <- validSet$call_counts
```

```{r}
# Convert categorical variables for LightGBM
for (col in categorical_vars) {
  train_x[[col]] <- as.numeric(as.factor(train_x[[col]])) - 1
  valid_x[[col]] <- as.numeric(as.factor(valid_x[[col]])) - 1
}
```


# Training a base learner using LightBoost


# Convert to LightGBM dataset format
```{r}
dtrain <- lgb.Dataset(data = as.matrix(train_x), label = train_y, categorical_feature = categorical_vars)
dvalid <- lgb.Dataset(data = as.matrix(valid_x), label = valid_y, categorical_feature = categorical_vars)
```


# Hyperparameter Optimization
```{r}
tune_hyperparameters <- function(dtrain, dvalid, nfolds = 5) {
  # Grid of parameters to try
  param_grid <- expand.grid(
    num_leaves = c(31, 63, 127),
    learning_rate = c(0.01, 0.05, 0.1),
    feature_fraction = c(0.7, 0.8, 0.9),
    min_data_in_leaf = c(20, 50, 100),
    max_depth = c(-1, 5, 10)
  )
  
  best_score <- Inf
  best_params <- NULL
  
  for(i in 1:nrow(param_grid)) {
    current_params <- list(
      objective = "poisson",
      metric = "poisson",
      boosting = "gbdt",
      num_leaves = param_grid$num_leaves[i],
      learning_rate = param_grid$learning_rate[i],
      feature_fraction = param_grid$feature_fraction[i],
      min_data_in_leaf = param_grid$min_data_in_leaf[i],
      max_depth = param_grid$max_depth[i]
    )
    
    tryCatch({
      cv_results <- lgb.cv(
        params = current_params,
        data = dtrain,
        nfold = nfolds,
        nrounds = 1000,
        early_stopping_rounds = 50,
        stratified = TRUE,
        eval_freq = 50,
        verbose = 0
      )
      best_score_cv <- cv_results$best_score
      
      if(best_score_cv < best_score) {
        best_score <- best_score_cv
        best_params <- current_params
        cat(sprintf("New best score: %.4f with parameters:\n", best_score))
        print(best_params)
        cat("\n")
      }
    }, error = function(e) {
      cat(sprintf("Error with parameters at row %d: %s\n", i, e$message))
    })
  }
  
  if (is.null(best_params)) {
    stop("No valid parameters found during tuning")
  }
  
  return(best_params)
}
```

# Model Training
```{r}
# Train Model
train_model <- function(dtrain, dvalid, params, seed = 42) {
  set.seed(seed)
  model <- lgb.train(
    params = params,
    data = dtrain,
    nrounds = 1000,
    valids = list(training = dtrain, valid = dvalid),
    early_stopping_rounds = 50,
    eval_freq = 10,
    verbose = 1
  )
  return(model)
}
```

# Model Evaluation
```{r}
evaluate_model <- function(model, X, y = NULL) {
  # Make predictions
  preds <- predict(model, as.matrix(X), num_iteration = model$best_iter)
  
  if (!is.null(y)) {
    # Calculate metrics
    metrics <- calculate_metrics(y, preds)
    
    # Feature importance
    importance <- lgb.importance(model, percentage = TRUE)
    importance_plot <- lgb.plot.importance(importance, top_n = 10, measure = "Gain")
    
    # Training history plot
    history_plot <- plot_training_history(model)
    
    # Actual vs Predicted plot
    pred_plot <- ggplot(data.frame(Actual = y, Predicted = preds), 
                        aes(x = Actual, y = Predicted)) +
      geom_point(alpha = 0.5) +
      geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
      labs(title = "Actual vs Predicted Values", 
           x = "Actual Call Counts", 
           y = "Predicted Call Counts") +
      theme_minimal()
    
    return(list(
      metrics = metrics,
      importance = importance,
      plots = list(
        importance = importance_plot,
        history = history_plot,
        predictions = pred_plot
      )
    ))
  } else {
    # Return only predictions for test data
    return(list(
      predictions = preds
    ))
  }
}
```

# Main Execution
```{r}
# Tune hyperparameters
best_params <- tune_hyperparameters(dtrain, dvalid)
```

# Train model with best parameters
```{r}
model <- train_model(dtrain, dvalid, best_params)
```

# Evaluate model on training data
```{r}
train_results <- evaluate_model(model, train_x, train_y)
print(train_results$metrics)
```

# Evaluate model on test data
```{r}
test_results <- evaluate_model(model, imputed_data_test)  
print(head(test_results$predictions, 10))
```

# Convert test predictions to a data frame
```{r}
test_predictions_df <- data.frame(Predictions = test_results$predictions)

# Convert test predictions to a data frame with 'id' and 'predict' columns
submission <- data.frame(
  id = seq_len(nrow(test_data)),                 
  Predict = test_predictions_df$Predictions 
)
```

# Print first few rows of the submission file
```{r}
print("Sample Submission Format (first 10 rows):")
print(head(submission, 10))
```





##### Define a function for the genetic algorithm to optimize 

```{r}
optimize_lgbm <- function(params) {
  num_leaves <- round(params[1])
  learning_rate <- params[2]
  feature_fraction <- params[3]
  min_data_in_leaf <- round(params[4])
  max_depth <- round(params[5])
  
  # Set parameters with feature_pre_filter = FALSE
  current_params <- list(
    objective = "poisson",
    metric = "poisson",
    boosting = "gbdt",
    num_leaves = num_leaves,
    learning_rate = learning_rate,
    feature_fraction = feature_fraction,
    min_data_in_leaf = min_data_in_leaf,
    max_depth = max_depth,
    feature_pre_filter = FALSE
  )
  
  # Cross-validation for the current set of parameters
  cv_results <- lgb.cv(
    params = current_params,
    data = dtrain,
    nfold = 5,
    nrounds = 1000,
    early_stopping_rounds = 50,
    verbose = -1
  )
  
  # Extract the best validation error
  best_eval <- min(unlist(cv_results$record_evals$valid$poisson$eval))
  
  # Return the validation error to minimize
  return(best_eval)
}

# Run the Genetic Algorithm
ga_result <- ga(
  type = "real-valued",
  fitness = optimize_lgbm,
  lower = c(31, 0.01, 0.7, 20, -1),       # lower bounds for parameters
  upper = c(127, 0.1, 0.9, 100, 10),      # upper bounds for parameters
  popSize = 20,                            # population size
  maxiter = 50,                            # maximum number of iterations
  run = 10,                                # early stopping condition
  seed = 123
)

# Extract best hyperparameters found by GA
best_params <- list(
  num_leaves = round(ga_result@solution[1]),
  learning_rate = ga_result@solution[2],
  feature_fraction = ga_result@solution[3],
  min_data_in_leaf = round(ga_result@solution[4]),
  max_depth = round(ga_result@solution[5])
)

# Train the final model using these best parameters
model <- train_model(dtrain, dvalid, best_params)

best_params$metric <- "poisson"

model <- lgb.train(
  params = best_params,
  data = dtrain,
  nrounds = 1000,
  valids = list(training = dtrain, valid = dvalid),
  early_stopping_rounds = 50,
  eval_freq = 10,
  verbose = 1
)

# Updated function for plotting training history
plot_training_history <- function(model) {
  # Check if record_evals has valid metrics
  if (!is.null(model$record_evals$valid$poisson$eval)) {
    data <- data.frame(
      Iteration = 1:length(model$record_evals$valid$poisson$eval),
      Training = model$record_evals$training$poisson$eval,
      Validation = model$record_evals$valid$poisson$eval
    )
    
    ggplot(data, aes(x = Iteration)) +
      geom_line(aes(y = Training, color = "Training")) +
      geom_line(aes(y = Validation, color = "Validation")) +
      labs(title = "Training History",
           y = "Poisson Loss",
           color = "Dataset") +
      theme_minimal()
  } else {
    message("No validation metrics found in record_evals. Skipping history plot.")
    NULL
  }
}

# Updated evaluate_model function
evaluate_model <- function(model, X, y = NULL) {
  # Make predictions
  preds <- predict(model, as.matrix(X), num_iteration = model$best_iter)
  
  if (!is.null(y)) {
    # Calculate metrics
    metrics <- calculate_metrics(y, preds)
    
    # Feature importance
    importance <- lgb.importance(model, percentage = TRUE)
    importance_plot <- lgb.plot.importance(importance, top_n = 10, measure = "Gain")
    
    # Training history plot (handling missing metrics)
    history_plot <- plot_training_history(model)
    
    # Actual vs Predicted plot
    pred_plot <- ggplot(data.frame(Actual = y, Predicted = preds), 
                        aes(x = Actual, y = Predicted)) +
      geom_point(alpha = 0.5) +
      geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
      labs(title = "Actual vs Predicted Values", 
           x = "Actual Call Counts", 
           y = "Predicted Call Counts") +
      theme_minimal()
    
    return(list(
      metrics = metrics,
      importance = importance,
      plots = list(
        importance = importance_plot,
        history = history_plot,
        predictions = pred_plot
      )
    ))
  } else {
    # Return only predictions for test data
    return(list(
      predictions = preds
    ))
  }
}


# Evaluate model on training data
train_resultsn <- evaluate_model(model, train_x, train_y)
print("Training Metrics:")
print(train_resultsn$metrics)


# Evaluate model on test data
test_results <- evaluate_model(model, imputed_data_test)  
print("Test Predictions (first 10):")
print(head(test_results$predictions, 10))

# Convert test predictions to a data frame
test_predictions_df <- data.frame(Predictions = test_results$predictions)

# Convert test predictions to a data frame with 'id' and 'predict' columns
submission <- data.frame(
  id = seq_len(nrow(test_data)),                 
  Predict = test_predictions_df$Predictions 
)

# Print first few rows of the submission file
print("Sample Submission Format (first 10 rows):")
print(head(submission, 10))

# Optionally, save the submission to a CSV file
write.csv(submission, "submissionGA.csv", row.names = FALSE)








